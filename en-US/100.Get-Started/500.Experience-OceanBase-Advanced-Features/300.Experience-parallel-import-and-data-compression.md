# Experience parallel import and data compression

<a name="Dd1W2"></a>

## Parallel import
In addition to analytical queries, another important part of operational online analytical processing (OLAP) lies in the parallel import of massive amounts of data, namely the batch data processing capability. The parallel execution framework of OceanBase Database allows you to execute data manipulation language (DML) statements in parallel, which is known as parallel DML (PDML) operations. It supports concurrent data writes to multiple servers in a multi-node database without compromising data consistency for large transactions. The asynchronous minor compaction mechanism enhances the support of the LSM-tree-based storage engine for large transactions when memory is tight.<br />This article describes how to experience the PDML feature of OceanBase Database. First, create an empty table named **lineitem2** with the same schema as that of the **lineitem** table used in running the TPC-H benchmark. Then, execute the `insert into ... select` statement to insert 6 million rows of data of the lineitem table into the **lineitem2** table. In the following examples, the statement is executed with PDML disabled and enabled respectively for comparison. 

First, copy the schema of the lineitem table and create an empty table named **lineitem2**. Table partitioning is used to manage large tables in OceanBase Database. In this example, 16 partitions are used. Therefore, the **lineitem2** table must also be split into 16 partitions:
```sql
MySQL [test]> show create table lineitem\G;
*************************** 1. row ***************************
       Table: lineitem

Create Table: CREATE TABLE `lineitem` (
  `l_orderkey` bigint(20) NOT NULL,
  `l_partkey` bigint(20) NOT NULL,
  `l_suppkey` bigint(20) NOT NULL,
  `l_linenumber` bigint(20) NOT NULL,
  `l_quantity` bigint(20) NOT NULL,
  `l_extendedprice` bigint(20) NOT NULL,
  `l_discount` bigint(20) NOT NULL,
  `l_tax` bigint(20) NOT NULL,
  `l_returnflag` char(1) DEFAULT NULL,
  `l_linestatus` char(1) DEFAULT NULL,
  `l_shipdate` date NOT NULL,
  `l_commitdate` date DEFAULT NULL,
  `l_receiptdate` date DEFAULT NULL,
  `l_shipinstruct` char(25) DEFAULT NULL,
  `l_shipmode` char(10) DEFAULT NULL,
  `l_comment` varchar(44) DEFAULT NULL,
  PRIMARY KEY (`l_orderkey`, `l_linenumber`),
  KEY `I_L_ORDERKEY` (`l_orderkey`) BLOCK_SIZE 16384 LOCAL,
  KEY `I_L_SHIPDATE` (`l_shipdate`) BLOCK_SIZE 16384 LOCAL
) DEFAULT CHARSET = utf8mb4 ROW_FORMAT = COMPACT COMPRESSION = 'zstd_1.3.8' REPLICA_NUM = 1 BLOCK_SIZE = 16384 USE_BLOOM_FILTER = FALSE TABLET_SIZE = 134217728 PCTFREE = 0 TABLEGROUP = 'x_tpch_tg_lineitem_order_group'
 partition by key(l_orderkey)
(partition p0,
partition p1,
partition p2,
partition p3,
partition p4,
partition p5,
partition p6,
partition p7,
partition p8,
partition p9,
partition p10,
partition p11,
partition p12,
partition p13,
partition p14,
partition p15)
1 row in set (0.002 sec)
```

<a name="vHaR4"></a>

### Execute the statement with PDML disabled
PDML is disabled by default. After you create the lineitem2 table, execute the statement with PDML disabled. Because this is a large transaction involving 6 million rows of data, you must increase the default transaction timeout interval (in Î¼s) of OceanBase Database:
```sql
set ob_query_timeout = 1000000000;  
set ob_trx_timeout = 1000000000;  
insert into lineitem2 select * from lineitem;
```
Execution results:
```sql
MySQL [test]> insert into lineitem2 select * from lineitem;
Query OK, 6001215 rows affected (1 min 47.312 sec)
Records: 6001215  Duplicates: 0  Warnings: 0
```
![image.png](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer/V3.1.4/en-US/1.Get-Started/5.Experience-OceanBase-Advanced-Features/3.Experience-parallel-import-and-data-compression.md/import1.png)<br />The results show that **OceanBase Database takes 107s** to insert 6 million rows of data in a single transaction when PDML is disabled.
<a name="Jbd15"></a>

### Execute the statement with PDML enabled
Add a hint to the statement to enable PDML. Before you execute the statement again, clear the data inserted last time.
```sql
truncate table lineitem2;
insert /*+ parallel(16) enable_parallel_dml */ into lineitem2 select * from lineitem;
```
Execution results:
```sql
MySQL [test]> truncate table lineitem2;
Query OK, 0 rows affected (0.108 sec)

MySQL [test]> insert /*+ parallel(16) enable_parallel_dml */ into lineitem2 select * from lineitem;
Query OK, 6001215 rows affected (22.117 sec)
Records: 6001215  Duplicates: 0  Warnings: 0
```
![image.png](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer/V3.1.4/en-US/1.Get-Started/5.Experience-OceanBase-Advanced-Features/3.Experience-parallel-import-and-data-compression.md/import2.png)<br />The results show that the time required by OceanBase Database to insert 6 million rows of data into the same table is reduced to about 22 seconds when PDML is enabled. After the PDML feature is enabled, **the database performance is about 5 times the performance when this feature is disabled**. This feature can be useful in scenarios where you want to process data in batches.
<a name="TOMQQ"></a>

## Data compression
OceanBase Database has its own storage engine based on the LSM-tree structure. Data is roughly classified into baseline data and incremental data. Baseline data is stored in SSTables on the disk, and incremental data is stored in MemTables in the memory. In this way, data can be stored on the disk more compactly. In addition, baseline data on the disk is not frequently updated, and OceanBase Database re-compresses baseline data based on a general compression algorithm. Therefore, data stored in OceanBase Database has a high compression ratio. However, data compression does not degrade the query or write performance. This section describes how to import a large amount of data to OceanBase Database and check the data compression ratio.

<a name="jgFNt"></a>

### Generate data
First, use the CreateData.jar tool to generate 50 million rows of test data in the /home/soft directory. To download the tool, click [ðŸ“ŽCreateData.jar](https://yuque.antfin.com/attachments/lark/0/2022/jar/99622/1659402075309-3195cacf-c820-4d3a-98b0-ef1b6f4d2bf5.jar). It takes about 10 minutes to generate the data. You can also use other tools to generate test data.
```shell
#mkdir /home/soft/
#java -jar CreateData.jar /home/soft/ 50000000
filePath is : /home/soft/
Start Time : 2022-07-10 15:33:00
End Time : 2022-07-10 15:52:53
#du -sh *
10G     t_bigdata_loader.unl
```

OceanBase Database allows you to import data from CSV files in multiple ways. This section describes how to run the **load data** command to import data.<br />First, name the generated file and check the actual file size.
```sql
# mv t_bigdata_loader.unl t_f1.csv
# du -sh t_f1.csv
10G     t_f1.csv

```

The following example shows the content of the test file **t_f1.csv** generated by the tool by using a random algorithm. The file has eight data columns of different data types. Before you perform data compression, you must create a table in the tenant and import the records from the CSV file into the table.
```shell
1|1896404182|1980-06-01|2004-10-25 13:30:39|7470.689|33062564.9527|nOLqnBYtnp|BzWYjZjeodtBNzXSMyBduMNzwDPSiVmhVgPJMeEkeAwKBCorzblwovIHDKBsQhbVjQnIdoeTsiLXTNwyuAcuneuNaol|
2|572083440|2018-11-09|1998-07-11 01:23:28|6891.054|66028434.4013|UzqteeMaHP|vQWbWBXEWgUqUTzqsOSciiOuvWVcZSrlEOQDwDVGmvGRQYWmhCFdEkpsUsqrWEpKtmxSwURHIHxvmlXHUIxmfelYboeGEuScKKqzpuNLryFsStaFTTRqSsVlCngFFjHnEnpaCnWsdwztbiHJyoGkaxrFmyPAmVregfydArrUZsgRqBpQ|
3|1139841892|2006-10-07|1999-06-26 17:02:22|286.43692|51306547.5055|KJJtylgxkv|BuBdFTBIIFsEPVxsVBRqAnFXSBdtZDgfumUhIx|
4|1777342512|1982-12-18|2017-11-19 07:56:35|2986.242|85860387.8696|rTkUBWhdPt|JSazOTAmvtCBrINttDwublNJNRFDIiWkHtWZXmWgKHoZCKGqmmETkIcYLXiSgKkoaATNgjvPxVGjeCOODLEWqrQHqowbMjOLOKrtirWEOpUSxiUudZduTCUvZElKzZfggvCBNthwzKJc|
....
```
Create a table named **t_f1** in the **test** database of the **test** tenant. For more information about how to create a tenant, see [Create a tenant](https://www.oceanbase.com/en/docs/community/observer-en/V3.1.1/10000000000209904).
```shell
obclient -h127.0.0.1 -P2881 -uroot@test  -Dtest -A -p -c

obclient [test]>create table t_f1(id decimal(10,0),id2 decimal(10,0),id3 date,id4 date,id5 float,id6 float,id7 varchar(30),id8 varchar(300));

```
<a name="cVbl6"></a>

### Import data
Run the built-in load data command of OceanBase Database to import data. The load data command also supports parallel import. Before you import data, complete the following settings. The load data command allows you to import data only from a local OBServer. If you want to import remote data, you can use the OBLoader tool of OceanBase Database.
```sql
set ob_query_timeout=1000000000;
set ob_trx_timeout=1000000000;
set GLOBAL secure_file_priv = "";
grant file on *.* to username;
```

After you complete the settings, reconnect to the session to make the settings take effect. Then, run the **load data** command.
```sql
load data /*+ parallel(16) */ infile '/home/soft/t_f1.csv' into table t_f1 fields terminated by '\|' lines terminated by '\n';
```
![image.png](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer/V3.1.4/en-US/1.Get-Started/5.Experience-OceanBase-Advanced-Features/3.Experience-parallel-import-and-data-compression.md/import3.png
)<br />The results show that OceanBase Database takes about 4 minutes to import 10 GB of data when parallel import is enabled. In this example, the tenant is configured with 16 CPU cores. You can set an appropriate degree of parallelism (DOP) based on your configurations. A higher DOP indicates faster data import.<br />After the data is imported, check the number of records in the table and the space occupied by the table.

- The table contains 50 million records.
  ```sql
  obclient [test]> select count(*) from t_f1;
  +----------+
  | count(*) |
  +----------+
  | 50000000 |
  +----------+
  ```

- Trigger a major freeze

  To check the compression results of baseline data, log on to your OceanBase database as the administrator of the sys tenant and trigger a major freeze to compact and compress the incremental data and baseline data. You can execute the following SQL statement to trigger a major freeze.
  ```sql
  obclient -h127.0.0.1 -P2881 -uroot@sys  -Doceanbase -A -p -c
  obclient[oceanbase]> ALTER SYSTEM MAJOR FREEZE;
  ```

  Execute the following SQL statement to query the status of the major freeze. If the statement returns IDLE, the major freeze is completed.
  ```sql
  MySQL [oceanbase]> select name,info from __all_zone where zone='' and name = 'merge_status';
  +--------------+------+
  | name         | info |
  +--------------+------+
  | merge_status | IDLE |
  +--------------+------+
  1 row in set (0.001 sec)
  ```

  Execute the following SQL statement in the sys tenant to query the space occupied by the imported data:
  ```sql
  obclient [oceanbase]> select b.table_name,svr_ip,role,a.data_size/1024/1024/1024 from __all_virtual_meta_table a,__all_virtual_table b where a.role=1 and a.table_id=b.table_id and b.table_name='T_F1';
  +------------+---------------+------+----------------------------+
  | table_name | svr_ip        | role | a.data_size/1024/1024/1024 |
  +------------+---------------+------+----------------------------+
  | t_f1       | xxx.xx.xxx.xx |    1 |             6.144531250000 |
  +------------+---------------+------+----------------------------+

  ```
  The compressed table is about 6.145 GB in size, and the compression ratio is 10/6.145 = 1.62. After the compression, the data size only takes about 61.4% space of original data.


