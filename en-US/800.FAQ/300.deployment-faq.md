# Deployment

## What are the system requirements for installing an OceanBase server?

The following table lists the minimum system requirements for installing OceanBase servers.

| **Server type** | **Quantity** | **Minimum functional configuration** | **Minimum performance configuration** |
| --- | --- | --- | --- |
| OCP RootServer | 1 | 32-core, 128 GB memory, and 1.5 TB storage | 32-core, 128 GB memory, 1.5 TB SSD storage, and 10 Gbit/s NIC |
| OceanBase Database computing server | 3 | 32-core, 128 GB memory, and 1.2 TB storage | 32-core, 256 GB memory, 2 TB SSD storage, and 10 Gbit/s NIC |

To ensure the high availability of OCP root services, you need three OCP RootServers and a hardware- or software-based load balancer, such as F5, Alibaba Cloud Server Load Balancer (SLB), and the soft-load balancing component ob_dns of OceanBase to implement three-node deployment.
The following table describes the Linux operating systems that support OceanBase Database.

| Linux operating system | Version                                                                              | Server architecture |
|--------------------------|--------------------------------------------------------------------------------------|-------------------------------|
| AliOS | 7.2 and later                                                                        | x86_64 (including Hygon) or AArch64 (Kunpeng and Phytium) |
| Anolis OS | 8.6 and later                                                                        | x86_64 (including Hygon) or AArch64 (Kunpeng and Phytium) |
| Kylin OS | V10                                                                                  | x86_64 (including Hygon) or AArch64 (Kunpeng and Phytium) |
| UOS | V20                                                                                  | x86_64 (including Hygon) or AArch64 (Kunpeng and Phytium) |
| NFSChina | V4.0 and later                                                                       | x86_64 (including Hygon) or AArch64 (Kunpeng and Phytium) |
| Inspur KOS | V5.8                                                                                 | x86_64 (including Hygon) or AArch64 (Kunpeng and Phytium) |
| CentOS or Red Hat Enterprise Linux | 7.2 and later <blockquote><b>Note</b></br>CentOS 8.X is not supported. </blockquote> | x86_64 (including Hygon) or AArch64 (Kunpeng and Phytium) |

<main id="notice" type='explain'>
    <h4>Note</h4>
    <p>The network and YUM or Zypper must be configured for the operating system. </p>
  </main>

## How do I deploy OceanBase Database in the production environment?

The following table describes deployment solutions:

| **Solution** | **Feature** | **Infrastructure requirement** | **Scenario** |
| --- | --- | --- | --- |
| Three replicas in one IDC | This solution provides a zero recovery point objective (RPO) and low recovery time objective (RTO). It also enables automatic failover when the primary server fails.  This solution enables your application to recover from some hardware failures but not IDC failures or city-wide disasters.  | Single-IDC | This solution applies to scenarios where you do not expect your application to recover from IDC failures or city-wide disasters.  |
| Three replicas in three IDCs in the same region | This solution provides a zero recovery point objective (RPO) and low recovery time objective (RTO). It also enables automatic failover when the primary server fails.  This solution enables your application to recover from some hardware and IDC failures but not city-wide disasters.  | This solution requires three IDCs in the same region.  Low network latency between the IDCs is ensured.  | It applies to scenarios where you need your application to recover from IDC failures but do not expect it to recover from city-wide disasters.  |
| Five replicas in five IDCs across three regions | This solution provides a zero recovery point objective (RPO) and low recovery time objective (RTO). It also enables automatic failover when the primary server fails.  This solution enables your application to recover from some hardware failures, IDC failures, and city-wide disasters.  | Deploy five IDCs across three regions.  Two regions must be geographically close to provide low network latency.  | It applies to scenarios where you need your application to recover from IDC failures and city-wide disasters.  |
| Two IDCs in the same region + Inter-cluster data replication | The RPO of this solution is greater than zero and the RTO is high, but it enables you to determine whether to fail over when the primary server fails.  It enables your application to recover from some hardware and IDC failures but not city-wide disasters.  | This solution requires two IDCs in the same region.  | This solution applies when you have two IDCs in the same region and need your application to recover when either IDC fails.  |
| Five replicas in three IDCs across two regions + Inter-cluster data replication | IDC failure: This solution provides zero RPO and low RTO. It also enables automatic failover when the primary server fails.  City-wide disaster: The RPO of this solution is greater than zero and the RTO is high, but it enables you to determine whether to fail over when the primary server fails.  This solution enables your application to recover from some hardware failures, IDC failures, and city-wide disasters.  | This solution requires three IDCs deployed across two regions.  | This solution applies when you have three IDCs deployed across two regions and you need your application to recover from IDC failures and city-wide disasters.  |
